{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scikit-learn 1.4.0\n",
      "Uninstalling scikit-learn-1.4.0:\n",
      "  Successfully uninstalled scikit-learn-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping imblearn as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.2.2\n",
      "  Downloading scikit_learn-1.2.2-cp311-cp311-win_amd64.whl (8.3 MB)\n",
      "     ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/8.3 MB 660.6 kB/s eta 0:00:13\n",
      "     - -------------------------------------- 0.3/8.3 MB 3.2 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 0.7/8.3 MB 5.3 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.1/8.3 MB 6.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 1.6/8.3 MB 7.6 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 2.1/8.3 MB 8.2 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 2.6/8.3 MB 9.2 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 3.2/8.3 MB 9.7 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 3.7/8.3 MB 9.9 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 4.3/8.3 MB 10.5 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.9/8.3 MB 10.4 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 5.5/8.3 MB 10.9 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 6.0/8.3 MB 10.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 6.5/8.3 MB 11.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 7.1/8.3 MB 11.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 7.6/8.3 MB 11.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.2/8.3 MB 11.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 8.3/8.3 MB 11.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn==1.2.2) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn==1.2.2) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn==1.2.2) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn==1.2.2) (2.2.0)\n",
      "Installing collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-1.2.2\n",
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (from imblearn) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Installing collected packages: imblearn\n",
      "Successfully installed imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall scikit-learn --yes\n",
    "!pip uninstall imblearn --yes\n",
    "!pip install scikit-learn==1.2.2\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in c:\\users\\user\\anaconda3\\lib\\site-packages (11.0.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyarrow) (1.24.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install pyarrow\n",
    "%pip install numpy\n",
    "%pip install scikit-learn\n",
    "%pip install imbalanced-learn\n",
    "%pip install matplotlib\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "filepath = \"./data/datathon.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting unnecessary and combining similar variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29200\\2503592472.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_everbought['ever_bought_lh_sum'] = df_everbought.filter(like='lh').sum(axis=1)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29200\\2503592472.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_lastbought_lh['last_bought_lh_min'] = df_lastbought_lh.apply(lambda row: min(row.dropna(), default=np.nan), axis=1)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29200\\2503592472.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_lastbought_lh['last_bought_lh_min'] = df_lastbought_lh['last_bought_lh_min'].replace('9999', '0', regex = True)\n"
     ]
    }
   ],
   "source": [
    "df_orig = pd.read_parquet('catB_train.parquet')\n",
    "df = df_orig.copy()\n",
    "\n",
    "exclude = [\"grp\", \"ltc\", \"gi\", \"ape\", \"sumin\", \"prempaid\", \"inv\",\n",
    "           \"consent\", \"valid\", \"ctry\", \"pr\",\n",
    "           \"mindef\", \"elx\", \"retail\", \"housewife\",\n",
    "           \"mail\", \"hold\", \"hh_20\", \"pop_20\", \"hh_size\",\n",
    "           \"affconnect\", \"n_months_since_visit_affcon\", \"clmcon_visit_days\",\n",
    "           \"claim\", \"index\", \"recency\", \"class\", \"type\", \"clntnum\", \"at_least\", \"occ\"]\n",
    "\n",
    "df1 = df.loc[:, df.apply(lambda x: len(x.unique()) > 1)]\n",
    "\n",
    "exclude_cols = [col for col in df1.columns if any(substring in col for substring in exclude)]\n",
    "df1 = df1.drop(columns=exclude_cols)\n",
    "\n",
    "df1 = df1.loc[:, ~df1.columns.str.contains(\"last_bought\") | df1.columns.str.contains(\"last_bought_lh\")]\n",
    "df1 = df1.loc[:, ~df1.columns.str.contains(\"ever_bought\") | df1.columns.str.contains(\"ever_bought_lh\")]\n",
    "df1 = df1.loc[:, ~df1.columns.str.contains(\"lapse\") | df1.columns.str.contains(\"flg_latest_being_lapse\")]\n",
    "\n",
    "\n",
    "df_everbought = df1.loc[:, df1.columns.str.contains(\"ever_bought_lh\")]\n",
    "df_everbought['ever_bought_lh_sum'] = df_everbought.filter(like='lh').sum(axis=1)\n",
    "df_everbought = df_everbought[['ever_bought_lh_sum']]\n",
    "\n",
    "df_lastbought_lh = df1.filter(like=\"last_bought_lh\")\n",
    "df_lastbought_lh['last_bought_lh_min'] = df_lastbought_lh.apply(lambda row: min(row.dropna(), default=np.nan), axis=1)\n",
    "df_lastbought_lh['last_bought_lh_min'] = df_lastbought_lh['last_bought_lh_min'].replace('9999', '0', regex = True)\n",
    "df_lastbought_lh = df_lastbought_lh[['last_bought_lh_min']]\n",
    "\n",
    "columns_to_drop = df1.columns[df1.columns.str.contains('ever_bought|last_bought')]\n",
    "df2 = df1.drop(columns=columns_to_drop, errors='ignore')\n",
    "df2 = pd.concat([df2, df_lastbought_lh], axis=1, join='inner')\n",
    "df2 = pd.concat([df2, df_everbought], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Delete columns with all entries the same (no differentiating factors).\n",
    "2. Delete columns with headers containing ‘grp’, ‘ltc’, ‘inv’, and ‘gi’ (group, long term care, investment, general) which are unrelated to health and life insurance.\n",
    "3. Delete columns with headers containing ‘elx’, ‘mindef’, and ‘retail’ where specific programs or sectors contribute little to chances of one buying insurance. \n",
    "4. Delete columns related to contact information (containing ‘mail’, ‘email’, ‘call’, ‘sms’, etc.) that have low association with one buying insurance.  \n",
    "5. Delete columns we deem as irrelevant, i.e. ‘ctrycode’ and ‘is_sg_pr‘’. \n",
    "\n",
    "6. Combine the five columns that begin with ‘n_months_last_bought_lh_<product_code>,' representing five different life and health insurance products, into a single column named 'last_bought_lh_min.' Use the minimum value across all products as the indicator of the recent purchase history of the client.\n",
    "7. Combine the five columns that begin with ‘f_ever_bought_lh_<product_code>,' representing five different life and health insurance products, into a single column named 'ever_bought_lh_sum.' Summing the values across all products as the indicator of the total number of purchases of the client.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the date of birthe variable to age(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29200\\314704808.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[missing_mask] = np.random.choice(value_counts.index, size=missing_mask.sum(), p=value_counts.values)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29200\\314704808.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[missing_mask] = np.random.choice(value_counts.index, size=missing_mask.sum(), p=value_counts.values)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29200\\314704808.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[missing_mask] = np.random.choice(value_counts.index, size=missing_mask.sum(), p=value_counts.values)\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.copy().drop('f_purchase_lh', axis = 1)\n",
    "df3['f_ever_declined_la'] = df3['f_ever_declined_la'].fillna(0)\n",
    "columns_with_missing_values = df3.columns[df3.isnull().any()]\n",
    "\n",
    "def calculate_age(date_of_birth):\n",
    "    # Parse the date of birth string into a datetime object\n",
    "    dob_date = datetime.strptime(date_of_birth, '%Y-%m-%d')\n",
    "\n",
    "    # Get the current date\n",
    "    current_date = datetime.now()\n",
    "\n",
    "    # Calculate the age\n",
    "    age = current_date.year - dob_date.year - ((current_date.month, current_date.day) < (dob_date.month, dob_date.day))\n",
    "\n",
    "    return age\n",
    "\n",
    "df3['cltdob_fix'] = df3['cltdob_fix'].apply(lambda x: calculate_age(x) if x != 'None' else None)\n",
    "\n",
    "bins = [0, 25, 60, float('inf')]\n",
    "labels = [0, 1, 2]\n",
    "\n",
    "# Create a new column 'age_range' based on the age ranges\n",
    "df3['cltdob_fix'] = pd.cut(df3['cltdob_fix'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "df3 = df3.rename(columns={'cltdob_fix': 'age'})\n",
    "\n",
    "def fill_missing_randomly(column):\n",
    "    # Calculate the percentage of each value in the column\n",
    "    value_counts = column.value_counts(normalize=True)\n",
    "\n",
    "    # Replace missing values with values based on the current distribution\n",
    "    missing_mask = column.isnull()\n",
    "    column[missing_mask] = np.random.choice(value_counts.index, size=missing_mask.sum(), p=value_counts.values)\n",
    "\n",
    "for col in columns_with_missing_values:\n",
    "  fill_missing_randomly(df3[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change the ‘cltdob_fix’ column to ‘age’ column. Convert the client's date of birth to their age and classify it into three categories: below 25 ->0; 25 to 60 -> 1; and above 60 -> 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing all the categorial data into integer representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df3['annual_income_est'] = label_encoder.fit_transform(df3['annual_income_est'])\n",
    "df3['stat_flag'] = label_encoder.fit_transform(df3['stat_flag'])\n",
    "df3['age'] = label_encoder.fit_transform(df3['age'])\n",
    "df3['race_desc'] = label_encoder.fit_transform(df3['race_desc'])\n",
    "df3['cltsex_fix'] = label_encoder.fit_transform(df3['cltsex_fix'])\n",
    "\n",
    "df3['f_purchase_lh'] = df2['f_purchase_lh']\n",
    "df3['f_purchase_lh'] = df3['f_purchase_lh'].fillna(0)\n",
    "\n",
    "df3 = df3.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change the ‘cltsex_fix’ column to binary expression. I.e Male = 0, Female = 1\n",
    "2. Change the ‘annual_income_est’ ranges to numbers: ‘Below 30k’ -> 0; ‘30k-60k’ -> 1; ‘60k - 100k’ -> 2;  ‘100k-200k’ -> 3;  ‘Above 200k’ -> 4.\n",
    "3. The above transformations are used to change all categorical variables we want to include in our model to number expressions so that SMOTE can be used to deal with the imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SMOTE to handle inbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df3['f_purchase_lh']\n",
    "X = df3.drop('f_purchase_lh', axis=1)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85      2802\n",
      "           1       0.84      0.86      0.85      2732\n",
      "\n",
      "    accuracy                           0.85      5534\n",
      "   macro avg       0.85      0.85      0.85      5534\n",
      "weighted avg       0.85      0.85      0.85      5534\n",
      "\n",
      "Cross-Validation Scores:\n",
      "[0.8230936  0.86194434 0.85833032 0.85902765 0.86101572]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_smote, y_train_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# 5-fold cross validation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "cv_scores = cross_val_score(rf_classifier, X_train_smote, y_train_smote, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-Validation Scores:\")\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    result = [] \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filepath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This cell should output a list of predictions.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(filepath)\n\u001b[0;32m      3\u001b[0m test_df \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf_purchase_lh\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(testing_hidden_data(test_df))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filepath' is not defined"
     ]
    }
   ],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
